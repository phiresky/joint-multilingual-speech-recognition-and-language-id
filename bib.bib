
@article{to17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.01694},
  primaryClass = {cs, eess},
  title = {Multilingual {{Speech Recognition With A Single End}}-{{To}}-{{End Model}}},
  url = {http://arxiv.org/abs/1711.01694},
  abstract = {Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21\% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7\% relative and eliminate confusion between different languages.},
  urldate = {2018-07-01},
  date = {2017-11-05},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing},
  author = {Toshniwal, Shubham and Sainath, Tara N. and Weiss, Ron J. and Li, Bo and Moreno, Pedro and Weinstein, Eugene and Rao, Kanishka},
  file = {/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/J995VBL7/Toshniwal et al. - 2017 - Multilingual Speech Recognition With A Single End-.pdf;/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/P7NP5M69/1711.html}
}

@article{wa17,
  title = {Hybrid {{CTC}}/{{Attention Architecture}} for {{End}}-to-{{End Speech Recognition}}},
  volume = {11},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2017.2763455},
  abstract = {Conventional automatic speech recognition (ASR) based on a hidden Markov model (HMM)/deep neural network (DNN) is a very complicated system consisting of various modules such as acoustic, lexicon, and language models. It also requires linguistic resources, such as a pronunciation dictionary, tokenization, and phonetic context-dependency trees. On the other hand, end-to-end ASR has become a popular alternative to greatly simplify the model-building process of conventional ASR systems by representing complicated modules with a single deep network architecture, and by replacing the use of linguistic resources with a data-driven learning method. There are two major types of end-to-end architectures for ASR; attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC) uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes hybrid CTC/attention end-to-end ASR, which effectively utilizes the advantages of both architectures in training and decoding. During training, we employ the multiobjective learning framework to improve robustness and achieve fast convergence. During decoding, we perform joint decoding by combining both attention-based and CTC scores in a one-pass beam search algorithm to further eliminate irregular alignments. Experiments with English (WSJ and CHiME-4) tasks demonstrate the effectiveness of the proposed multiobjective learning over both the CTC and attention-based encoder-decoder baselines. Moreover, the proposed method is applied to two large-scale ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and exhibits performance that is comparable to conventional DNN/HMM ASR systems based on the advantages of both multiobjective learning and joint decoding without linguistic resources.},
  number = {8},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  date = {2017-12},
  pages = {1240-1253},
  keywords = {speech recognition,acoustic frames,attention mechanism,Automatic speech recognition,computational linguistics,connectionist temporal classification,conventional ASR systems,conventional automatic speech recognition,conventional DNN/HMM ASR systems,deep neural network,dynamic programming,end-to-end,end-to-end architectures,end-to-end speech recognition,hidden Markov model,hidden Markov models,Hidden Markov models,hybrid CTC/attention,hybrid CTC/attention architecture,hybrid CTC/attention end-to-end ASR,joint decoding,large-scale ASR benchmarks,learning (artificial intelligence),linguistic resources,Machine learning,Markov processes,model-building process,multiobjective learning framework,neural nets,Neural networks,one-pass beam search algorithm,Probabilistic logic,search problems,signal classification,single deep network architecture,speech coding},
  author = {Watanabe, S. and Hori, T. and Kim, S. and Hershey, J. R. and Hayashi, T.},
  file = {/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/X9LX7NG5/8068205.html}
}

@inproceedings{wa17a,
  title = {Language Independent End-to-End Architecture for Joint Language Identification and Speech Recognition},
  doi = {10.1109/ASRU.2017.8268945},
  abstract = {End-to-end automatic speech recognition (ASR) can significantly reduce the burden of developing ASR systems for new languages, by eliminating the need for linguistic information such as pronunciation dictionaries. This also creates an opportunity, which we fully exploit in this paper, to build a monolithic multilingual ASR system with a language-independent neural network architecture. We present a model that can recognize speech in 10 different languages, by directly performing grapheme (character/chunked-character) based speech recognition. The model is based on our hybrid attention/connectionist temporal classification (CTC) architecture which has previously been shown to achieve the state-of-the-art performance in several ASR benchmarks. Here we augment its set of output symbols to include the union of character sets appearing in all the target languages. These include Roman and Cyrillic Alphabets, Arabic numbers, simplified Chinese, and Japanese Kanji/Hiragana/Katakana characters (5,500 characters in all). This allows training of a single multilingual model, whose parameters are shared across all the languages. The model can jointly identify the language and recognize the speech, automatically formatting the recognized text in the appropriate character set. The experiments, which used speech databases composed of Wall Street Journal (English), Corpus of Spontaneous Japanese, HKUST Mandarin CTS, and Voxforge (German, Spanish, French, Italian, Dutch, Portuguese, Russian), demonstrate comparable/superior performance relative to language-dependent end-to-end ASR systems.},
  eventtitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  booktitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  date = {2017-12},
  pages = {265-271},
  keywords = {ASR benchmarks,ASR systems,character/chunked-character,Decoding,End-to-end ASR,end-to-end automatic speech recognition,Hidden Markov models,hybrid attention/connectionist temporal classification architecture,hybrid attention/CTC,Japanese Kanji/Hiragana/Katakana characters,joint language identification,language identification,language independent end-to-end architecture,language-independent architecture,language-independent neural network architecture,monolithic multilingual ASR system,multilingual ASR,natural language processing,Neural networks,simplified Chinese characters,single multilingual model,Speech,speech databases,speech recognition,Speech recognition,text analysis,Text recognition,Training},
  author = {Watanabe, S. and Hori, T. and Hershey, J. R.},
  file = {/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/DEM7C5VX/8268945.html}
}

@article{mi13,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3781},
  primaryClass = {cs},
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  url = {http://arxiv.org/abs/1301.3781},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  urldate = {2018-08-20},
  date = {2013-01-16},
  keywords = {Computer Science - Computation and Language},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  file = {/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/WPYNRFJF/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/JJU6CJBH/1301.html}
}

@article{vgg,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  url = {http://arxiv.org/abs/1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  urldate = {2018-08-20},
  date = {2014-09-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew},
  file = {/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/VUQN8B8F/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf;/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/VNHD9V54/1409.html}
}

@inproceedings{gr06,
  title = {Connectionist Temporal Classification: {{Labelling}} Unsegmented Sequence Data with Recurrent Neural Networks},
  shorttitle = {Connectionist Temporal Classification},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN. 1.},
  booktitle = {In {{Proceedings}} of the {{International Conference}} on {{Machine Learning}}, {{ICML}} 2006},
  date = {2006},
  pages = {369--376},
  author = {Graves, Alex and Fern√°ndez, Santiago and Gomez, Faustino},
  file = {/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/WDV5BGAD/Graves et al. - 2006 - Connectionist temporal classification Labelling u.pdf;/home/phire/.zotero/zotero/08d2rslj.default/zotero/storage/WAT9C4XA/summary.html}
}


